{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Emotion Recognition - Model Training\n",
    "\n",
    "This notebook trains a **MobileNetV2-based CNN** to classify facial expressions into one of **7 emotion categories**:\n",
    "\n",
    "| Index | Emotion   |\n",
    "|-------|-----------|\n",
    "| 0     | Angry     |\n",
    "| 1     | Disgust   |\n",
    "| 2     | Fear      |\n",
    "| 3     | Happy     |\n",
    "| 4     | Sad       |\n",
    "| 5     | Surprise  |\n",
    "| 6     | Neutral   |\n",
    "\n",
    "## Approach\n",
    "\n",
    "We use **transfer learning** with a pre-trained MobileNetV2 backbone from ImageNet. The final classifier layer is replaced with a custom head that outputs 7 classes. This allows the model to leverage powerful learned features while adapting to our specific emotion recognition task.\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "The training images should be organized in the following folder structure:\n",
    "\n",
    "```\n",
    "data/images/\n",
    "    angry/\n",
    "        img001.jpg\n",
    "        img002.jpg\n",
    "        ...\n",
    "    disgust/\n",
    "        ...\n",
    "    fear/\n",
    "        ...\n",
    "    happy/\n",
    "        ...\n",
    "    sad/\n",
    "        ...\n",
    "    surprise/\n",
    "        ...\n",
    "    neutral/\n",
    "        ...\n",
    "```\n",
    "\n",
    "Each subfolder name must match one of the 7 emotion labels. Images can be `.jpg`, `.jpeg`, or `.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports\n",
    "\n",
    "Import all required libraries. The key dependencies are:\n",
    "- **PyTorch** for model definition, training, and inference\n",
    "- **torchvision** for pre-trained models and image transforms\n",
    "- **PIL / Pillow** for image loading\n",
    "- **matplotlib / seaborn** for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available:  False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:             {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "All training hyper-parameters and paths are defined here for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"data/images\"           # Root folder containing emotion sub-folders\n",
    "CHECKPOINT_DIR = \"checkpoints\"      # Where to save trained model weights\n",
    "MODEL_SAVE_PATH = os.path.join(CHECKPOINT_DIR, \"face_model.pth\")\n",
    "\n",
    "# Emotion labels (must match sub-folder names in DATA_DIR)\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "NUM_CLASSES = len(EMOTIONS)\n",
    "\n",
    "# Training hyper-parameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "VAL_SPLIT = 0.2                     # 20% of data used for validation\n",
    "\n",
    "# Image pre-processing\n",
    "IMAGE_SIZE = 224                     # MobileNetV2 expects 224x224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Class\n",
    "\n",
    "A custom PyTorch `Dataset` that:\n",
    "1. Scans each emotion sub-folder for images\n",
    "2. Assigns integer labels (0-6) based on the `EMOTIONS` list\n",
    "3. Applies image transforms on-the-fly\n",
    "4. Handles corrupt images gracefully by skipping to the next sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading face emotion images.\n",
    "    \n",
    "    Expects folder structure:\n",
    "        root_dir/\n",
    "            angry/   -> images\n",
    "            disgust/ -> images\n",
    "            ...      -> images\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_EXTENSIONS = ('.jpg', '.jpeg', '.png')\n",
    "    \n",
    "    def __init__(self, root_dir: str, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []   # list of file paths\n",
    "        self.labels = []   # list of integer labels\n",
    "        \n",
    "        for idx, emotion in enumerate(EMOTIONS):\n",
    "            emotion_path = os.path.join(root_dir, emotion)\n",
    "            if not os.path.exists(emotion_path):\n",
    "                print(f\"  [WARNING] Folder not found: {emotion_path}\")\n",
    "                continue\n",
    "            \n",
    "            count = 0\n",
    "            for img_name in os.listdir(emotion_path):\n",
    "                if img_name.lower().endswith(self.SUPPORTED_EXTENSIONS):\n",
    "                    self.images.append(os.path.join(emotion_path, img_name))\n",
    "                    self.labels.append(idx)\n",
    "                    count += 1\n",
    "            print(f\"  {emotion:>10s}: {count:,} images\")\n",
    "        \n",
    "        print(f\"\\n  Total: {len(self.images):,} images across {NUM_CLASSES} classes\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        try:\n",
    "            image = Image.open(self.images[idx]).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            # If an image is corrupt, skip to the next one\n",
    "            print(f\"  [WARNING] Could not load {self.images[idx]}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Transforms & Loading\n",
    "\n",
    "We define separate transforms for training and validation:\n",
    "\n",
    "- **Training**: Random horizontal flip + random rotation + color jitter for data augmentation\n",
    "- **Validation**: Only resize and normalize (no augmentation)\n",
    "\n",
    "Both normalize using ImageNet statistics since MobileNetV2 was pre-trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "  [WARNING] Folder not found: data/images/angry\n",
      "  [WARNING] Folder not found: data/images/disgust\n",
      "  [WARNING] Folder not found: data/images/fear\n",
      "  [WARNING] Folder not found: data/images/happy\n",
      "  [WARNING] Folder not found: data/images/sad\n",
      "  [WARNING] Folder not found: data/images/surprise\n",
      "  [WARNING] Folder not found: data/images/neutral\n",
      "\n",
      "  Total: 0 images across 7 classes\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ERROR: No images found! Check your data/images/ folder structure.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3014091232.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ERROR: No images found! Check your data/images/ folder structure.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: ERROR: No images found! Check your data/images/ folder structure."
     ]
    }
   ],
   "source": [
    "# Training transforms (with data augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Load full dataset (using train transforms initially for scanning)\n",
    "print(\"Loading dataset...\\n\")\n",
    "full_dataset = FaceDataset(DATA_DIR, transform=train_transform)\n",
    "\n",
    "assert len(full_dataset) > 0, \"ERROR: No images found! Check your data/images/ folder structure.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train / Validation Split\n",
    "\n",
    "We split the dataset into training and validation sets. The validation set is used to monitor overfitting and select the best model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "val_size = int(len(full_dataset) * VAL_SPLIT)\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "# Override the validation subset's transform to remove augmentation\n",
    "# (random_split shares the underlying dataset, so we apply val_transform at inference)\n",
    "# We'll handle this by creating separate datasets if needed, or applying transforms inline.\n",
    "\n",
    "print(f\"Training samples:   {train_size:,}\")\n",
    "print(f\"Validation samples: {val_size:,}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"Training batches:   {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dataset Distribution\n",
    "\n",
    "Visualize the class distribution to check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "label_counts = Counter(full_dataset.labels)\n",
    "labels_sorted = [EMOTIONS[i] for i in range(NUM_CLASSES)]\n",
    "counts_sorted = [label_counts.get(i, 0) for i in range(NUM_CLASSES)]\n",
    "\n",
    "# Plot distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#FF4444', '#FF9800', '#9C27B0', '#4CAF50', '#2196F3', '#00BCD4', '#9E9E9E']\n",
    "bars = ax.bar(labels_sorted, counts_sorted, color=colors, edgecolor='white', linewidth=0.5)\n",
    "ax.set_xlabel('Emotion', fontsize=12)\n",
    "ax.set_ylabel('Number of Images', fontsize=12)\n",
    "ax.set_title('Dataset Class Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, counts_sorted):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "            f'{count:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sample Images Preview\n",
    "\n",
    "Display a few sample images from the dataset to verify loading is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample images (one per class)\n",
    "fig, axes = plt.subplots(1, NUM_CLASSES, figsize=(20, 4))\n",
    "fig.suptitle('Sample Images (one per emotion)', fontsize=14, fontweight='bold')\n",
    "\n",
    "shown = set()\n",
    "for img_path, label in zip(full_dataset.images, full_dataset.labels):\n",
    "    if label not in shown:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB').resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            axes[label].imshow(img)\n",
    "            axes[label].set_title(EMOTIONS[label], fontsize=11, fontweight='bold',\n",
    "                                  color=colors[label])\n",
    "            axes[label].axis('off')\n",
    "            shown.add(label)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if len(shown) == NUM_CLASSES:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Architecture\n",
    "\n",
    "We use **MobileNetV2** pre-trained on ImageNet as the backbone. The architecture:\n",
    "\n",
    "1. **Feature Extractor**: MobileNetV2 convolutional layers (frozen initially, or fine-tuned)\n",
    "2. **Classifier Head**: Dropout (0.2) + Linear layer mapping to 7 emotion classes\n",
    "\n",
    "MobileNetV2 is lightweight and efficient, making it ideal for real-time emotion detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEmotionCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Face Emotion Classification model based on MobileNetV2.\n",
    "    \n",
    "    Uses transfer learning from ImageNet pre-trained weights.\n",
    "    The final classifier is replaced with a custom head for\n",
    "    7-class emotion classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 7):\n",
    "        super(FaceEmotionCNN, self).__init__()\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.mobilenet = models.mobilenet_v2(\n",
    "            weights=models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "        # Replace the classifier head\n",
    "        self.mobilenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(self.mobilenet.last_channel, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mobilenet(x)\n",
    "\n",
    "\n",
    "# Instantiate and move to device\n",
    "model = FaceEmotionCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model on device:      {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Loss Function & Optimizer\n",
    "\n",
    "- **Loss**: CrossEntropyLoss (standard for multi-class classification)\n",
    "- **Optimizer**: Adam with learning rate scheduling\n",
    "- **Scheduler**: ReduceLROnPlateau reduces the learning rate when validation loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer:     Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"Scheduler:     ReduceLROnPlateau (factor=0.5, patience=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Loop\n",
    "\n",
    "The training loop:\n",
    "1. Iterates over epochs\n",
    "2. For each epoch, trains on all batches and computes training loss/accuracy\n",
    "3. Evaluates on the validation set\n",
    "4. Saves the best model (lowest validation loss)\n",
    "5. Applies learning rate scheduling based on validation loss\n",
    "\n",
    "Training history is stored for later visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training\n",
    "# ============================================================\n",
    "\n",
    "# History for plotting\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [],   'val_acc': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ----- Training phase -----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "                  f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = 100.0 * correct / total\n",
    "    \n",
    "    # ----- Validation phase -----\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / val_total\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        save_marker = \" ** SAVED **\"\n",
    "    else:\n",
    "        save_marker = \"\"\n",
    "    \n",
    "    print(f\"\\n  Epoch {epoch+1}/{NUM_EPOCHS} Complete\"\n",
    "          f\"  |  Train Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\"\n",
    "          f\"  |  Val Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\"\n",
    "          f\"  |  LR: {current_lr:.6f}{save_marker}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Training History Visualization\n",
    "\n",
    "Plot loss and accuracy curves for both training and validation to diagnose model performance and detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "# --- Loss ---\n",
    "ax1.plot(epochs_range, history['train_loss'], 'o-', label='Train Loss', color='#FF4444', linewidth=2)\n",
    "ax1.plot(epochs_range, history['val_loss'], 's-', label='Val Loss', color='#2196F3', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Accuracy ---\n",
    "ax2.plot(epochs_range, history['train_acc'], 'o-', label='Train Accuracy', color='#4CAF50', linewidth=2)\n",
    "ax2.plot(epochs_range, history['val_acc'], 's-', label='Val Accuracy', color='#FF9800', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Evaluation - Confusion Matrix\n",
    "\n",
    "Generate predictions on the validation set and plot a confusion matrix to see which emotions are most confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=ax,\n",
    "            linewidths=0.5, linecolor='white')\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Confusion Matrix (Validation Set)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Classification Report\n",
    "\n",
    "Per-class precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(all_labels, all_preds, target_names=EMOTIONS, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Export Model for Production\n",
    "\n",
    "Copy the best checkpoint to the main project's `models/cv/` folder so the Streamlit app can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "PRODUCTION_PATH = os.path.join('..', 'models', 'cv', 'face_model.pth')\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    os.makedirs(os.path.dirname(PRODUCTION_PATH), exist_ok=True)\n",
    "    shutil.copy2(MODEL_SAVE_PATH, PRODUCTION_PATH)\n",
    "    print(f\"Model copied to: {PRODUCTION_PATH}\")\n",
    "    print(f\"File size: {os.path.getsize(PRODUCTION_PATH) / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(f\"ERROR: {MODEL_SAVE_PATH} not found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Quick Inference Test\n",
    "\n",
    "Test the trained model on a few random validation samples to visually verify predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# Inverse normalization for display\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-m/s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD)],\n",
    "    std=[1.0/s for s in IMAGENET_STD],\n",
    ")\n",
    "\n",
    "# Get a batch from validation set\n",
    "dataiter = iter(val_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(images.to(DEVICE))\n",
    "    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "# Display first 8 samples\n",
    "n_show = min(8, len(images))\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(n_show * 2.5, 6))\n",
    "fig.suptitle('Inference Test (top: image, bottom: probabilities)', fontsize=13, fontweight='bold')\n",
    "\n",
    "for i in range(n_show):\n",
    "    # Show image\n",
    "    img = inv_normalize(images[i]).permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    axes[0, i].imshow(img)\n",
    "    \n",
    "    pred_label = EMOTIONS[preds[i].item()]\n",
    "    true_label = EMOTIONS[labels[i].item()]\n",
    "    is_correct = pred_label == true_label\n",
    "    title_color = '#4CAF50' if is_correct else '#FF4444'\n",
    "    axes[0, i].set_title(f\"P: {pred_label}\\nT: {true_label}\",\n",
    "                          fontsize=9, color=title_color, fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Show probability bar\n",
    "    p = probs[i].cpu().numpy()\n",
    "    bar_colors = [colors[j] for j in range(NUM_CLASSES)]\n",
    "    axes[1, i].barh(EMOTIONS, p, color=bar_colors)\n",
    "    axes[1, i].set_xlim(0, 1)\n",
    "    axes[1, i].tick_params(axis='y', labelsize=7)\n",
    "    axes[1, i].tick_params(axis='x', labelsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Loaded face emotion dataset organized by emotion folders |\n",
    "| 2 | Applied data augmentation (flips, rotation, color jitter) |\n",
    "| 3 | Split into 80% train / 20% validation |\n",
    "| 4 | Used MobileNetV2 with transfer learning from ImageNet |\n",
    "| 5 | Trained with Adam optimizer + LR scheduling |\n",
    "| 6 | Evaluated with confusion matrix and classification report |\n",
    "| 7 | Exported best model to `models/cv/face_model.pth` |\n",
    "\n",
    "The exported model is used by the main Streamlit application for real-time face emotion detection in uploaded videos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
