import os

os.environ.setdefault("STREAMLIT_SERVER_FILE_WATCHER_TYPE", "none")

import streamlit as st
import numpy as np
import pandas as pd
import librosa
import pickle
import tensorflow as tf
import joblib
import whisper
import torch
import tempfile
import time
import cv2
from pathlib import Path
from audio_formatter import convert_to_wav
from torchvision import transforms
from models.face_model import FaceEmotionCNN, EMOTIONS
from video_processor import process_video_emotions


try:
    torch_classes = getattr(torch, "classes", None)
    if torch_classes is not None:
        if not hasattr(torch_classes, "__path__"):
            class _TorchClassesPath(list):
                def __init__(self):
                    super().__init__()
                    self._path = []

                def __iter__(self):
                    return iter(self._path)

            torch_classes.__path__ = _TorchClassesPath()
        elif not hasattr(torch_classes.__path__, "_path"):
            torch_classes.__path__._path = []
    del torch_classes
except Exception:
    # Safe to continue even if patching torch classes fails.
    pass

BASE_DIR = Path(__file__).resolve().parent
TEXT_MODEL_DIR = BASE_DIR / "Emotion-Detection-in-Text"
TEXT_MODEL_PATH = TEXT_MODEL_DIR / "models" / "emotion_classifier_pipe_lr.pkl"
# Set page config
st.set_page_config(page_title="Emotion Recognition from Speech & Video", page_icon="üéß", layout="wide", initial_sidebar_state="collapsed")

# Custom CSS for dark theme styling
st.markdown("""
    <style>
    .main {
        padding: 2rem 1rem;
        background-color: #0E1117;
    }
    .stApp {
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
    }
    [data-testid="stAppViewContainer"] {
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
    }
    .title-container {
        background: rgba(26, 26, 46, 0.8);
        padding: 2rem;
        border-radius: 20px;
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
        margin-bottom: 2rem;
        text-align: center;
        border: 1px solid rgba(102, 126, 234, 0.3);
    }
    .emotion-card {
        background: rgba(26, 26, 46, 0.8);
        padding: 1.5rem;
        border-radius: 15px;
        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.4);
        margin: 1rem 0;
        border: 1px solid rgba(102, 126, 234, 0.2);
    }
    .emotion-label {
        font-size: 2.5rem;
        font-weight: bold;
        margin: 0.5rem 0;
        text-transform: uppercase;
        letter-spacing: 2px;
    }
    .confidence-label {
        font-size: 2rem;
        font-weight: bold;
        margin: 0.5rem 0;
    }
    .section-header {
        font-size: 1.8rem;
        font-weight: bold;
        margin: 1.5rem 0 1rem 0;
        color: #a0a0d0;
    }
    .upload-section {
        background: rgba(26, 26, 46, 0.8);
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.4);
        margin: 2rem 0;
        border: 1px solid rgba(102, 126, 234, 0.2);
    }
    .transcription-box {
        background: rgba(15, 52, 96, 0.6);
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #667eea;
        font-size: 1.1rem;
        line-height: 1.6;
        color: #e0e0e0;
    }
    .final-result {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 20px;
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
        text-align: center;
        margin: 2rem 0;
        border: 1px solid rgba(255, 255, 255, 0.2);
    }
    .final-emotion {
        font-size: 3rem;
        font-weight: bold;
        margin: 1rem 0;
        text-transform: uppercase;
        letter-spacing: 3px;
    }
    .final-confidence {
        font-size: 2.5rem;
        font-weight: bold;
        margin: 1rem 0;
    }
    /* Dark theme for Streamlit components */
    .stMarkdown {
        color: #e0e0e0;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #e0e0e0 !important;
    }
    p {
        color: #b0b0b0 !important;
    }
    </style>
""", unsafe_allow_html=True)

# üéµ Title Section
st.markdown(
    """
    <div class="title-container">
        <h1 style='color: #a0a0ff; margin-bottom: 0.5rem;'>üéß Emotion Recognition from Speech & Video</h1>
        <p style='font-size: 1.2rem; color: #b0b0d0; margin-top: 0.5rem;'>Upload audio files to detect voice emotions, or video files to detect face emotions!</p>
    </div>
    """,
    unsafe_allow_html=True
)

# üîÑ Load model and dependencies
@st.cache_resource
def load_model_and_utils():
    try:
        model = tf.keras.models.load_model("models/emotion_recognition_model.keras")
        scaler_params = np.load("models/scaler_params.npy", allow_pickle=True).item()
        mean = scaler_params['mean']
        scale = scaler_params['scale']
        with open("models/label_encoder.pkl", "rb") as f:
            le = pickle.load(f)
        return model, mean, scale, le
    except Exception as e:
        st.error(f"‚ùå Error loading model or dependencies: {e}")
        return None, None, None, None

model, mean, scale, le = load_model_and_utils()


@st.cache_resource
def load_text_emotion_model():
    try:
        return joblib.load(TEXT_MODEL_PATH)
    except Exception as e:
        st.error(f"‚ùå Error loading text emotion model: {e}")
        return None


@st.cache_resource
def load_whisper_model():
    model_name = os.getenv("WHISPER_MODEL_NAME", "small.en")
    try:
        return whisper.load_model(model_name)
    except Exception as e:
        st.error(f"‚ùå Error loading Whisper model '{model_name}': {e}")
        return None


pipe_lr = load_text_emotion_model()
whisper_model = load_whisper_model()


@st.cache_resource
def load_face_emotion_model():
    """Load face emotion detection model."""
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = FaceEmotionCNN(num_classes=7)
        model_path = BASE_DIR / "models" / "face_model.pth"
        
        if model_path.exists():
            model.load_state_dict(torch.load(str(model_path), map_location=device))
        model.to(device)
        model.eval()
        return model, device
    except Exception as e:
        st.error(f"‚ùå Error loading face emotion model: {e}")
        return None, None


face_model, face_device = load_face_emotion_model()

# Face image transform
face_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])


TEXT_TO_VOICE_EMOTION_MAP = {
    "anger": "angry",
    "joy": "happy",
    "fear": "fear",
    "disgust": "disgust",
    "sadness": "sad",
    "shame": "neutral",
    "surprise": "surprise",
}


emotion_colors = {
    "angry": "#FF4444",
    "happy": "#4CAF50",
    "sad": "#2196F3",
    "fear": "#9C27B0",
    "disgust": "#FF9800",
    "surprise": "#00BCD4",
    "neutral": "#9E9E9E",
}


def map_text_emotion_to_voice(emotion_label: str) -> str:
    return TEXT_TO_VOICE_EMOTION_MAP.get(emotion_label, emotion_label)


def aggregate_voice_probabilities(prob_matrix, classes):
    voice_probabilities = {}
    for cls, prob in zip(classes, prob_matrix[0]):
        voice_label = map_text_emotion_to_voice(cls)
        voice_probabilities[voice_label] = voice_probabilities.get(voice_label, 0.0) + float(prob)
    return voice_probabilities


def aggregate_sentiment_labels(audio_label: str, audio_conf: float, text_label: str, text_conf: float):
    w_audio = 0.72
    w_text = 0.28
    if audio_label == text_label:
        final_label = audio_label
        final_conf = (w_audio * audio_conf + w_text * text_conf) / (w_audio + w_text)
    else:
        audio_score = w_audio * audio_conf
        text_score = w_text * text_conf
        if audio_score > text_score:
            final_label = audio_label
            final_conf = audio_score / (w_audio + w_text)
        else:
            final_label = text_label
            final_conf = text_score / (w_audio + w_text)
    return final_label, round(final_conf, 3)

# üéØ Feature Extraction
def extract_features(file_path, target_len=40):
    try:
        audio, sr = librosa.load(file_path, sr=22050)
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
        if mfcc.shape[1] < target_len:
            pad_width = target_len - mfcc.shape[1]
            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')
        else:
            mfcc = mfcc[:, :target_len]
        return mfcc
    except Exception as e:
        st.error(f"‚ùå Error extracting features: {e}")
        return None

# üîÅ Delta & Delta¬≤
def enhance_features(X):
    delta = np.diff(X, axis=1)
    delta = np.pad(delta, ((0, 0), (1, 0)), mode='edge')
    delta2 = np.diff(delta, axis=1)
    delta2 = np.pad(delta2, ((0, 0), (1, 0)), mode='edge')
    return np.stack([X, delta, delta2], axis=0)

# üîç Prediction
def predict_audio_emotion(file):
    try:
        if model is None:
            return None, None, {}
        features = extract_features(file)
        if features is None:
            return None, None, {}

        enhanced = enhance_features(features)

        mfcc_std = (enhanced[0] - mean[:, np.newaxis]) / scale[:, np.newaxis]
        delta_std = (enhanced[1] - mean[:, np.newaxis]) / scale[:, np.newaxis]
        delta2_std = (enhanced[2] - mean[:, np.newaxis]) / scale[:, np.newaxis]

        enhanced_std = np.stack([mfcc_std, delta_std, delta2_std], axis=0)
        enhanced_std = enhanced_std[:, :, :40]
        input_data = np.expand_dims(enhanced_std[:, :, 0], axis=0)

        preds = model.predict(input_data, verbose=0)
        predicted_index = np.argmax(preds)
        emotion = le.inverse_transform([predicted_index])[0]
        confidence = float(np.max(preds))
        
        # Create probability dictionary for all emotions
        all_emotions = le.classes_
        audio_probabilities = {}
        for idx, prob in enumerate(preds[0]):
            emotion_name = le.inverse_transform([idx])[0]
            audio_probabilities[emotion_name] = float(prob)
        
        return emotion, confidence, audio_probabilities
    except Exception as e:
        st.error(f"‚ùå Prediction error: {e}")
        return None, None, {}


def transcribe_audio(file_path: str) -> str:
    if whisper_model is None:
        return ""
    try:
        result = whisper_model.transcribe(file_path, language="en")
        return result.get("text", "").strip()
    except Exception as e:
        st.error(f"‚ùå Transcription error: {e}")
        return ""


def predict_text_emotion(text: str):
    if not text or pipe_lr is None:
        return None, None, {}
    try:
        prediction = pipe_lr.predict([text])[0]
        mapped_prediction = map_text_emotion_to_voice(prediction)
        probability = pipe_lr.predict_proba([text])
        voice_probabilities = aggregate_voice_probabilities(probability, pipe_lr.classes_)
        mapped_confidence = voice_probabilities.get(mapped_prediction, 0.0)
        return mapped_prediction, mapped_confidence, voice_probabilities
    except Exception as e:
        st.error(f"‚ùå Text emotion prediction error: {e}")
        return None, None, {}

# üì§ File Upload Section
st.markdown("""
    <div class="upload-section">
        <h2 style='text-align: center; color: #a0a0ff; margin-bottom: 1rem;'>üì§ Upload Your Audio File</h2>
    </div>
""", unsafe_allow_html=True)
uploaded_file = st.file_uploader(
    "Upload audio file",
    type=["wav", "mp3", "flac", "ogg", "m4a", "aac"],
    label_visibility="collapsed",
)

if uploaded_file is not None:
    file_ext = os.path.splitext(uploaded_file.name)[1].lower()

    if file_ext != ".wav":
        progress_bar = st.progress(0)
        status_text = st.empty()

        def update_progress(percent, text):
            progress_bar.progress(percent)
            status_text.text(text)

        wav_path = convert_to_wav(uploaded_file, progress_callback=update_progress)

        # Clear progress after done
        progress_bar.empty()
        status_text.empty()
    else:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(uploaded_file.read())
            wav_path = tmp_file.name

    if wav_path:
        st.markdown("<br>", unsafe_allow_html=True)
        st.audio(wav_path, format="audio/wav")

        with st.spinner("üîç Analyzing emotion from audio and text..."):
            time.sleep(1.2)
            audio_label, audio_confidence, audio_probabilities = predict_audio_emotion(wav_path)
            transcript = transcribe_audio(wav_path)
            text_label, text_confidence, text_probabilities = predict_text_emotion(transcript)

        if audio_label:
            st.markdown("<br>", unsafe_allow_html=True)
            st.markdown("""
                <div class="emotion-card">
                    <h2 class="section-header" style='color: #667eea;'>üîä Voice Emotion Analysis</h2>
                </div>
            """, unsafe_allow_html=True)
            
            audio_cols = st.columns([1, 1])
            emotion_color = emotion_colors.get(audio_label.lower(), "#667eea")

            with audio_cols[0]:
                st.markdown(
                    f"""
                    <div style='text-align: center; padding: 1rem; background: linear-gradient(135deg, {emotion_color}33 0%, {emotion_color}55 100%); border-radius: 10px; border: 1px solid {emotion_color}66;'>
                        <h3 style='color: #d0d0d0; margin-bottom: 0.5rem;'>Emotion</h3>
                        <h2 class="emotion-label" style='color: {emotion_color};'>{audio_label.upper()}</h2>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
            with audio_cols[1]:
                st.markdown(
                    f"""
                    <div style='text-align: center; padding: 1rem; background: linear-gradient(135deg, #4CAF5033 0%, #4CAF5055 100%); border-radius: 10px; border: 1px solid #4CAF5066;'>
                        <h3 style='color: #d0d0d0; margin-bottom: 0.5rem;'>Confidence</h3>
                        <h2 class="confidence-label" style='color: #4CAF50;'>{audio_confidence * 100:.2f}%</h2>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
            
            # Voice Probabilities Chart
            if audio_probabilities:
                st.markdown("<br>", unsafe_allow_html=True)
                st.markdown("### üìä Voice Emotion Probabilities")
                audio_proba_df = (
                    pd.DataFrame.from_dict(audio_probabilities, orient="index", columns=["probability"])
                    .sort_values("probability", ascending=False)
                )
                st.bar_chart(audio_proba_df, height=300, use_container_width=True)

        if transcript:
            st.markdown("<br>", unsafe_allow_html=True)
            st.markdown("""
                <div class="emotion-card">
                    <h2 class="section-header" style='color: #667eea;'>üìù Transcription</h2>
                    <div class="transcription-box">
                        {transcript}
                    </div>
                </div>
            """.format(transcript=transcript), unsafe_allow_html=True)

        if text_label:
            st.markdown("<br>", unsafe_allow_html=True)
            st.markdown("""
                <div class="emotion-card">
                    <h2 class="section-header" style='color: #667eea;'>üìö Text Emotion Analysis</h2>
                </div>
            """, unsafe_allow_html=True)
            
            text_cols = st.columns([1, 1])
            text_emotion_color = emotion_colors.get(text_label.lower(), "#667eea")

            with text_cols[0]:
                st.markdown(
                    f"""
                    <div style='text-align: center; padding: 1rem; background: linear-gradient(135deg, {text_emotion_color}22 0%, {text_emotion_color}44 100%); border-radius: 10px;'>
                        <h3 style='color: #555; margin-bottom: 0.5rem;'>Emotion</h3>
                        <h2 class="emotion-label" style='color: {text_emotion_color};'>{text_label.upper()}</h2>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
            with text_cols[1]:
                st.markdown(
                    f"""
                    <div style='text-align: center; padding: 1rem; background: linear-gradient(135deg, #4CAF5022 0%, #4CAF5044 100%); border-radius: 10px;'>
                        <h3 style='color: #555; margin-bottom: 0.5rem;'>Confidence</h3>
                        <h2 class="confidence-label" style='color: #4CAF50;'>{text_confidence * 100:.2f}%</h2>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

            # Text Probabilities Chart
            if text_probabilities:
                st.markdown("<br>", unsafe_allow_html=True)
                st.markdown("### üìä Text Emotion Probabilities")
                text_proba_df = (
                    pd.DataFrame.from_dict(text_probabilities, orient="index", columns=["probability"])
                    .sort_values("probability", ascending=False)
                )
                st.bar_chart(text_proba_df, height=300, use_container_width=True)
        else:
            if pipe_lr is None:
                st.warning("‚ö†Ô∏è Text emotion model unavailable. Check configuration.")
            elif transcript == "":
                if whisper_model is None:
                    st.warning("‚ö†Ô∏è Transcription disabled because Whisper model could not load.")
                else:
                    st.warning("‚ö†Ô∏è Transcription produced empty text. Try another recording.")

        if audio_label and text_label:
            final_label, final_confidence = aggregate_sentiment_labels(
                audio_label, audio_confidence, text_label, text_confidence
            )
            st.markdown("<br>", unsafe_allow_html=True)
            final_emotion_color = emotion_colors.get(final_label.lower(), "#667eea")
            st.markdown(f"""
                <div class="final-result">
                    <h2 style='margin-bottom: 1rem; font-size: 2rem;'>üß† Final Aggregated Emotion</h2>
                    <div class="final-emotion" style='color: white;'>{final_label.upper()}</div>
                    <div class="final-confidence" style='color: rgba(255, 255, 255, 0.9);'>{final_confidence * 100:.2f}%</div>
                </div>
            """, unsafe_allow_html=True)

        if not audio_label:
            st.warning("‚ö†Ô∏è Unable to detect voice emotion. Please try another file.")
    else:
        st.error("‚ùå Conversion failed. Please upload a valid audio file.")

# üé• Video Upload Section for Face Emotion Detection
st.markdown("<br><br>", unsafe_allow_html=True)
st.markdown("""
    <div class="upload-section">
        <h2 style='text-align: center; color: #a0a0ff; margin-bottom: 1rem;'>üé• Upload Your Video File for Face Emotion Detection</h2>
    </div>
""", unsafe_allow_html=True)

uploaded_video = st.file_uploader(
    "Upload video file",
    type=["mp4", "avi", "mov", "mkv", "webm"],
    key="video_uploader",
    label_visibility="collapsed",
)

if uploaded_video is not None:
    if face_model is None or face_device is None:
        st.error("‚ùå Face emotion model is not available. Please check model files.")
    else:
        # Save video to temporary file
        file_ext = os.path.splitext(uploaded_video.name)[1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp_video:
            tmp_video.write(uploaded_video.read())
            video_path = tmp_video.name
        
        # Display video
        st.markdown("<br>", unsafe_allow_html=True)
        st.video(video_path)
        
        with st.spinner("üîç Analyzing face emotions in video..."):
            try:
                # Process video
                results = process_video_emotions(
                    video_path,
                    face_model,
                    face_transform,
                    face_device,
                    fps_sample=1  # Sample 1 frame per second
                )
                
                if results['dominant_emotion'] is not None:
                    st.markdown("<br>", unsafe_allow_html=True)
                    st.markdown("""
                        <div class="emotion-card">
                            <h2 class="section-header" style='color: #667eea;'>üòä Face Emotion Analysis</h2>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    # Get dominant emotion name
                    dominant_emotion_name = EMOTIONS[results['dominant_emotion']]
                    
                    # Calculate average confidence
                    valid_confidences = [c for c in results['confidences'] if c > 0]
                    avg_confidence = np.mean(valid_confidences) if len(valid_confidences) > 0 else 0.0
                    
                    face_cols = st.columns([1, 1])
                    emotion_colors = {
                        "angry": "#FF4444",
                        "happy": "#4CAF50",
                        "sad": "#2196F3",
                        "fear": "#9C27B0",
                        "disgust": "#FF9800",
                        "surprise": "#00BCD4",
                        "neutral": "#9E9E9E"
                    }
                    emotion_color = emotion_colors.get(dominant_emotion_name.lower(), "#667eea")
                    
                    with face_cols[0]:
                        st.markdown(
                            f"""
                            <div style='text-align: center; padding: 1rem; background: linear-gradient(135deg, {emotion_color}33 0%, {emotion_color}55 100%); border-radius: 10px; border: 1px solid {emotion_color}66;'>
                                <h3 style='color: #d0d0d0; margin-bottom: 0.5rem;'>Dominant Emotion</h3>
                                <h2 class="emotion-label" style='color: {emotion_color};'>{dominant_emotion_name.upper()}</h2>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                    with face_cols[1]:
                        st.markdown(
                            f"""
                            <div style='text-align: center; padding: 1rem; background: linear-gradient(135deg, #4CAF5033 0%, #4CAF5055 100%); border-radius: 10px; border: 1px solid #4CAF5066;'>
                                <h3 style='color: #d0d0d0; margin-bottom: 0.5rem;'>Average Confidence</h3>
                                <h2 class="confidence-label" style='color: #4CAF50;'>{avg_confidence * 100:.2f}%</h2>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                    
                    # Emotion distribution chart
                    if results['avg_probabilities']:
                        st.markdown("<br>", unsafe_allow_html=True)
                        st.markdown("### üìä Face Emotion Probabilities")
                        face_proba_dict = {EMOTIONS[i]: results['avg_probabilities'][i] for i in range(len(EMOTIONS))}
                        face_proba_df = (
                            pd.DataFrame.from_dict(face_proba_dict, orient="index", columns=["probability"])
                            .sort_values("probability", ascending=False)
                        )
                        st.bar_chart(face_proba_df, height=300, use_container_width=True)
                    
                    # Emotion timeline
                    if len(results['frame_times']) > 0 and any(e is not None for e in results['emotions']):
                        st.markdown("<br>", unsafe_allow_html=True)
                        st.markdown("### üìà Emotion Timeline")
                        
                        # Create timeline data
                        timeline_data = []
                        for i, (time, emotion_idx) in enumerate(zip(results['frame_times'], results['emotions'])):
                            if emotion_idx is not None:
                                timeline_data.append({
                                    'Time (seconds)': time,
                                    'Emotion': EMOTIONS[emotion_idx],
                                    'Confidence': results['confidences'][i]
                                })
                        
                        if timeline_data:
                            timeline_df = pd.DataFrame(timeline_data)
                            
                            # Create emotion counts for pie chart
                            emotion_counts_df = pd.DataFrame({
                                'Emotion': [EMOTIONS[i] for i in results['emotion_counts'].keys()],
                                'Count': list(results['emotion_counts'].values())
                            })
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.line_chart(timeline_df.set_index('Time (seconds)')['Confidence'], height=300)
                            with col2:
                                st.bar_chart(emotion_counts_df.set_index('Emotion'), height=300)
                    
                    # Statistics
                    st.markdown("<br>", unsafe_allow_html=True)
                    st.markdown("### üìä Video Analysis Statistics")
                    stat_cols = st.columns(4)
                    total_frames = len(results['emotions'])
                    frames_with_faces = sum(1 for e in results['emotions'] if e is not None)
                    
                    with stat_cols[0]:
                        st.metric("Total Frames Analyzed", total_frames)
                    with stat_cols[1]:
                        st.metric("Frames with Faces", frames_with_faces)
                    with stat_cols[2]:
                        st.metric("Video Duration", f"{results['frame_times'][-1]:.1f}s" if results['frame_times'] else "0s")
                    with stat_cols[3]:
                        st.metric("Face Detection Rate", f"{frames_with_faces/total_frames*100:.1f}%" if total_frames > 0 else "0%")
                
                else:
                    st.warning("‚ö†Ô∏è No faces detected in the video. Please upload a video with visible faces.")
                
                # Clean up temporary file
                try:
                    os.remove(video_path)
                except:
                    pass
                    
            except Exception as e:
                st.error(f"‚ùå Error processing video: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                # Clean up temporary file
                try:
                    os.remove(video_path)
                except:
                    pass
